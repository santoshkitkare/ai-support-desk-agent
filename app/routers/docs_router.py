#######################################################################
# file: docs_router.py
# üöÄ docs_router.py ‚Äî The Document Onboarding Gateway
# Think of this router as your ‚ÄúDocument HR Department.‚Äù
# Every PDF, DOCX, CSV, or TXT entering the system gets vetted and handed off for processing.

# üìå What this module does:
# - Accepts uploaded documents via REST API
# - Validates incoming files
# - Injects them into the ingestion pipeline
# - Returns document IDs
# - Surfaces errors if extraction fails
# - Nothing fancy ‚Äî but it‚Äôs the front door to your knowledge base.
# 
# What This Router Actually Owns
# It handles:
# - Upload request validation
# - DB session injection
# - Delegating ingestion
# - Error reporting
# - Success response shaping

# It does NOT handle:
# - Parsing
# - Chunking
# - Embedding
# - FAISS indexing
# All of that is intentionally abstracted inside the ingestion service ‚Üí this keeps routers clean and maintainable.
#########################################################################
# 1Ô∏è‚É£ Import Layer
from fastapi import APIRouter, UploadFile, File, HTTPException, Depends
from typing import List

from sqlalchemy.orm import Session

from app.services.db import get_db
from app.services.ingestion_service import ingest_uploaded_files
"""
What‚Äôs happening:
- APIRouter ‚Üí allows modular endpoint grouping
- UploadFile + File ‚Üí handles streaming file uploads
- Depends(get_db) ‚Üí injects SQLAlchemy DB session
- ingest_uploaded_files ‚Üí triggers full ingestion pipeline
This router doesn‚Äôt do any heavy lifting ‚Äî it delegates.
"""
# 2Ô∏è‚É£ Router Initialization
# A fresh router group that later gets registered in main.py under: /docs
router = APIRouter()

# 3Ô∏è‚É£ Upload Endpoint Definition
# üîç Breakdown:
# - POST /docs/upload
# -Accepts multiple files (List[UploadFile])
# - DB session injected automatically
# - async allows efficient file streaming
# Why async matters:
# You‚Äôll avoid blocking the server when users upload large PDFs.
@router.post("/upload")
async def upload_docs(
    files: List[UploadFile] = File(...),
    db: Session = Depends(get_db),
):
    """
    Upload one or more documents (PDF, DOCX, CSV, TXT) to the knowledge base.
    Performs full ingestion: parse, chunk, embed, index in FAISS.
    """
    # 4Ô∏è‚É£ Validation ‚Äî Zero Files Check
    if not files:
        raise HTTPException(status_code=400, detail="No files uploaded")

    # 5Ô∏è‚É£ Core Processing Call
    # This triggers the entire pipeline:
    # üî• Inside ingestion service (what this call actually does):
    # 1. Reads PDFs, DOCX, CSV, TXT
    # 2. Extracts raw text
    # 3. Cleans text
    # 4. Chunks it
    # 5. Embeds chunks using BGE-small
    # 6. Saves chunks to DB
    # 7. Saves embeddings to FAISS
    # 8. Returns final document IDs
    # This is where the horsepower lives ‚Äî this router is just the dispatcher.
    doc_ids = ingest_uploaded_files(files, db)

    # 6Ô∏è‚É£ Handle No-Text Case
    # This prevents garbage documents (empty PDFs, image-only scans) from polluting your vector DB.
    if not doc_ids:
        raise HTTPException(status_code=400, detail="No valid text extracted from files")

    # 7Ô∏è‚É£ Final Success Response
    return {
        "message": "Documents ingested successfully",
        "document_ids": doc_ids,
        "count": len(doc_ids),
    }
