##############################################################################
# Filename: ğŸš€ chat_router.py â€” The Chat Orchestration Layer
# This module:
# - Accepts chat requests
# - Manages conversation sessions
# - Logs messages to DB
# - Sends queries to the RAG engine
# - Packages the response back to the UI
# It doesnâ€™t contain logic â€” it delegates to services.
# Thatâ€™s exactly how API layers should be: thin, clean, predictable.
# 
# What This Router Does Right

# âœ” Thin API layer (perfect)
# âœ” Delegates business logic to services
# âœ” Clean Pydantic models
# âœ” Manages conversation memory
# âœ” Logs user + assistant messages
# âœ” Supports multi-turn chat
# âœ” Works with /chat and /chat/
# âœ” CORS-safe with OPTIONS handler
##############################################################################
from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel
from typing import List, Optional

from sqlalchemy.orm import Session

from app.services.db import get_db
from app.services import chat_service
from app.services import conversation_service

router = APIRouter()


class ChatMessage(BaseModel):
    role: str  # "user" | "assistant" | "system"
    content: str

# ğŸ§± 1. Request/Response Models
# ğŸ“¥ ChatRequest
# Every user message must include:
# - session_id â†’ ties multiple messages to same conversation
# - message â†’ the actual user query
# Streamlit generates session_id once per browser.
class ChatRequest(BaseModel):
    session_id: str
    message: str

# ğŸ“¤ ChatResponse
# Returned back to UI:
# - answer â†’ LLMâ€™s final response
# - escalate_to_human â†’ if your prompt told LLM to escalate
# - context_docs â†’ file names of chunks used (for explainability)
# 
# Nice trick:
# Two decorators â†’ endpoint works with and without trailing /.
# Example:
# POST /chat
# POST /chat/
# This avoids frontend errors. Good move.
class ChatResponse(BaseModel):
    answer: str
    escalate_to_human: bool = False
    context_docs: Optional[List[str]] = None

# ğŸ”Œ 2. The Chat Endpoint
@router.post("", response_model=ChatResponse)
@router.post("/", response_model=ChatResponse)
async def chat(
    request: ChatRequest,
    db: Session = Depends(get_db),
):
    """
    Main chat endpoint with RAG:
    - Embed last user message
    - Retrieve relevant chunks from FAISS
    - Ask LLM with KB context
    - Return answer + escalate flag + context doc references
    """
    # ğŸ¯ 3. Chat Workflow Step-by-Step
    session_id = request.session_id
    user_msg = request.message

    # Step 1 â†’ Validate Input
    if not session_id or not user_msg:
        raise HTTPException(status_code=400, detail="session_id and message required")

    # Step 2 â†’ Get Conversation Record
    # If the session exists â†’ return it.
    # If not â†’ create a new conversation row.
    # This powers:
    # - History
    # - Analytics
    # - Multi-turn context
    # - Conversation logs
    # Get or create conversation
    conv = conversation_service.get_or_create_conversation(db, session_id)

    # Step 3 â†’ Save User Message
    # The DB now stores:
    # role=user
    # content=user_msg
    # timestamp
    # conversation_id
    # This is what populates your â€œconversation history UI.â€
    # Save user message
    conversation_service.add_message(
        db=db,
        conversation_id=conv.id,
        role="user",
        content=user_msg,
    )

    # Step 4 â†’ Fetch Last 10 Messages
    # This ensures the RAG + LLM can maintain context.
    # Youâ€™re doing sliding window of last 10 messages â€” perfect for support bots.
    # Load conversation history (last 10 messages)
    history = conversation_service.get_history(db, conv.id, last_n=10)

    # Step 5 â†’ RAG + LLM Pipeline
    # This function handles the heavy lifting:
    # embed query
    # search FAISS
    # rerank chunks (bge-reranker)
    # build prompt with context
    # call GPT-4o-mini
    # parse JSON
    # output answer + escalate flag
    # Run RAG + LLM
    result = chat_service.answer_with_rag(
        user_query=user_msg,
        history=history,
        db=db,
    )

    # Step 6 â†’ Store Assistant Reply
    # You are logging every AI answer â†’ great for analytics:
    # resolution rate
    # trending questions
    # escalations
    # satisfaction tracking
    # offline evaluations
    # Store assistant reply
    conversation_service.add_message(
        db=db,
        conversation_id=conv.id,
        role="assistant",
        content=result["answer"],
    )

    # Step 7 â†’ Return Response
    return ChatResponse(
        answer=result["answer"],
        escalate_to_human=result["escalate_to_human"],
        context_docs=result.get("context_docs", []),
    )
    

# ğŸ§¯ 8. OPTIONS Handler â†’ CORS Preflight
# This ensures browsers don't freak out when sending CORS preflight requests:
# Streamlit â†’ FastAPI
# POST â†’ requires OPTIONS preflight
# OPTIONS returns 200 OK
@router.options("/")
async def options_handler():
    return {}    # This makes the preflight return 200 OK
