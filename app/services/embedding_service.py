##############################################################################
# file: üöÄ embedding_service.py ‚Äî "The Vector Factory"
# This module handles:
# - Loading SentenceTransformer model
# - Generating embeddings (single + batch)
# - Normalizing vectors
# - GPU support (optional)
# Your RAG accuracy heavily depends on this file.
from typing import List
import torch
from sentence_transformers import SentenceTransformer
from app.services.config import settings

# Load once at startup
# If you want GPU and it's available: .to('cuda')
# üìå 1. Model Initialization
# ‚úî What‚Äôs happening:
# - Loads HuggingFace embedding model once at startup
# - Stores it in a module-level variable _model
# - Ensures max input sequence length = 512 tokens

# üß† Why this is awesome:
# - Faster inference: model is not reloaded per request
# - No reinitialization across uploads
# - Efficient memory usage
# - Production-grade design
_model = SentenceTransformer(settings.HF_EMBEDDING_MODEL)
_model.max_seq_length = 512  # safety for large chunks

# üìå 2. Embedding a Single Text
# Key behaviors:
# üîπ convert_to_numpy=True
# Returns NumPy arrays ‚Üí fast, clean, FAISS-compatible.
# üîπ normalize_embeddings=True
# Normalizes vectors to unit length ‚Üí essential for cosine similarity.
# This dramatically stabilizes retrieval quality.
# Output:
# A single vector ‚Üí likely 384 or 768 dimensions depending on model.
def get_embedding(text: str) -> List[float]:
    """
    Encode a single text into a vector (length 384).
    """
    emb = _model.encode(text, convert_to_numpy=True, normalize_embeddings=True)
    return emb.tolist()

# üìå 3. Batch Embeddings
# üî• Why batching matters:
# - Faster (vectorized computation)
# - Efficient on GPU or CPU
# - Required when processing hundreds of chunks during ingestion
# Good engineering choices:
# - batch_size=32 ‚Üí optimal for CPU
# - show_progress_bar=True ‚Üí useful during debugging
# Normalized embeddings improve FAISS performance
def get_embeddings(texts: List[str]) -> List[List[float]]:
    """
    Encode multiple texts into vectors (batch).
    """
    embs = _model.encode(texts, convert_to_numpy=True, normalize_embeddings=True, batch_size=32, show_progress_bar=True)
    return [vec.tolist() for vec in embs]



"""
‚ö° Opportunities for Enhancements (Optional)
Not required, but future upgrades:
1Ô∏è‚É£ Model quantization
Reduce memory footprint by ~40%.
2Ô∏è‚É£ Switch to bge-large-en for higher accuracy
If you have GPU.
3Ô∏è‚É£ Add async support
Not critical unless embedding thousands of docs at once.
4Ô∏è‚É£ Add caching layer
Avoid recomputing identical chunk embeddings.
"""